{"ast":null,"code":"'use strict';\n\nvar _objectSpread = require(\"/opt/work/NZ_test/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/objectSpread\");\n\nvar _asyncToGenerator = require(\"/opt/work/NZ_test/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/asyncToGenerator\");\n\nvar _regeneratorRuntime = require(\"/opt/work/NZ_test/node_modules/babel-preset-react-app/node_modules/@babel/runtime/regenerator\");\n\nvar _awaitAsyncGenerator = require(\"/opt/work/NZ_test/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/awaitAsyncGenerator\");\n\nvar _wrapAsyncGenerator = require(\"/opt/work/NZ_test/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/wrapAsyncGenerator\");\n\nfunction _asyncIterator(iterable) { var method, async, sync, retry = 2; for (\"undefined\" != typeof Symbol && (async = Symbol.asyncIterator, sync = Symbol.iterator); retry--;) { if (async && null != (method = iterable[async])) return method.call(iterable); if (sync && null != (method = iterable[sync])) return new AsyncFromSyncIterator(method.call(iterable)); async = \"@@asyncIterator\", sync = \"@@iterator\"; } throw new TypeError(\"Object is not async iterable\"); }\n\nfunction AsyncFromSyncIterator(s) { function AsyncFromSyncIteratorContinuation(r) { if (Object(r) !== r) return Promise.reject(new TypeError(r + \" is not an object.\")); var done = r.done; return Promise.resolve(r.value).then(function (value) { return { value: value, done: done }; }); } return AsyncFromSyncIterator = function AsyncFromSyncIterator(s) { this.s = s, this.n = s.next; }, AsyncFromSyncIterator.prototype = { s: null, n: null, next: function next() { return AsyncFromSyncIteratorContinuation(this.n.apply(this.s, arguments)); }, return: function _return(value) { var ret = this.s.return; return void 0 === ret ? Promise.resolve({ value: value, done: !0 }) : AsyncFromSyncIteratorContinuation(ret.apply(this.s, arguments)); }, throw: function _throw(value) { var thr = this.s.return; return void 0 === thr ? Promise.reject(value) : AsyncFromSyncIteratorContinuation(thr.apply(this.s, arguments)); } }, new AsyncFromSyncIterator(s); }\n\nvar errCode = require('err-code');\n\nvar _require = require('ipfs-unixfs'),\n    UnixFS = _require.UnixFS;\n\nvar persist = require('../../utils/persist');\n\nvar _require2 = require('ipld-dag-pb'),\n    DAGNode = _require2.DAGNode,\n    DAGLink = _require2.DAGLink;\n\nvar parallelBatch = require('it-parallel-batch');\n\nvar mh = require('multihashing-async').multihash;\n/**\n * @typedef {import('../../types').BlockAPI} BlockAPI\n * @typedef {import('../../types').File} File\n * @typedef {import('../../types').ImporterOptions} ImporterOptions\n * @typedef {import('../../types').Reducer} Reducer\n * @typedef {import('../../types').DAGBuilder} DAGBuilder\n * @typedef {import('../../types').FileDAGBuilder} FileDAGBuilder\n */\n\n/**\n * @type {{ [key: string]: FileDAGBuilder}}\n */\n\n\nvar dagBuilders = {\n  flat: require('./flat'),\n  balanced: require('./balanced'),\n  trickle: require('./trickle')\n};\n/**\n * @param {File} file\n * @param {BlockAPI} block\n * @param {ImporterOptions} options\n */\n\nfunction buildFileBatch(_x, _x2, _x3) {\n  return _buildFileBatch.apply(this, arguments);\n}\n/**\n * @param {File} file\n * @param {BlockAPI} block\n * @param {ImporterOptions} options\n */\n\n\nfunction _buildFileBatch() {\n  _buildFileBatch = _wrapAsyncGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee(file, block, options) {\n    var count, previous, bufferImporter, _iteratorAbruptCompletion, _didIteratorError, _iteratorError, _iterator, _step, entry;\n\n    return _regeneratorRuntime.wrap(function _callee$(_context) {\n      while (1) {\n        switch (_context.prev = _context.next) {\n          case 0:\n            count = -1;\n\n            if (typeof options.bufferImporter === 'function') {\n              bufferImporter = options.bufferImporter;\n            } else {\n              bufferImporter = require('./buffer-importer');\n            }\n\n            _iteratorAbruptCompletion = false;\n            _didIteratorError = false;\n            _context.prev = 4;\n            _iterator = _asyncIterator(parallelBatch(bufferImporter(file, block, options), options.blockWriteConcurrency));\n\n          case 6:\n            _context.next = 8;\n            return _awaitAsyncGenerator(_iterator.next());\n\n          case 8:\n            if (!(_iteratorAbruptCompletion = !(_step = _context.sent).done)) {\n              _context.next = 25;\n              break;\n            }\n\n            entry = _step.value;\n            count++;\n\n            if (!(count === 0)) {\n              _context.next = 16;\n              break;\n            }\n\n            previous = entry;\n            return _context.abrupt(\"continue\", 22);\n\n          case 16:\n            if (!(count === 1 && previous)) {\n              _context.next = 20;\n              break;\n            }\n\n            _context.next = 19;\n            return previous;\n\n          case 19:\n            previous = null;\n\n          case 20:\n            _context.next = 22;\n            return entry;\n\n          case 22:\n            _iteratorAbruptCompletion = false;\n            _context.next = 6;\n            break;\n\n          case 25:\n            _context.next = 31;\n            break;\n\n          case 27:\n            _context.prev = 27;\n            _context.t0 = _context[\"catch\"](4);\n            _didIteratorError = true;\n            _iteratorError = _context.t0;\n\n          case 31:\n            _context.prev = 31;\n            _context.prev = 32;\n\n            if (!(_iteratorAbruptCompletion && _iterator.return != null)) {\n              _context.next = 36;\n              break;\n            }\n\n            _context.next = 36;\n            return _awaitAsyncGenerator(_iterator.return());\n\n          case 36:\n            _context.prev = 36;\n\n            if (!_didIteratorError) {\n              _context.next = 39;\n              break;\n            }\n\n            throw _iteratorError;\n\n          case 39:\n            return _context.finish(36);\n\n          case 40:\n            return _context.finish(31);\n\n          case 41:\n            if (!previous) {\n              _context.next = 45;\n              break;\n            }\n\n            previous.single = true;\n            _context.next = 45;\n            return previous;\n\n          case 45:\n          case \"end\":\n            return _context.stop();\n        }\n      }\n    }, _callee, null, [[4, 27, 31, 41], [32,, 36, 40]]);\n  }));\n  return _buildFileBatch.apply(this, arguments);\n}\n\nvar reduce = function reduce(file, block, options) {\n  /**\n   * @type {Reducer}\n   */\n  function reducer(_x4) {\n    return _reducer.apply(this, arguments);\n  }\n\n  function _reducer() {\n    _reducer = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee2(leaves) {\n      var leaf, _yield$block$get, _buffer, multihash, f, links, node, buffer, cid;\n\n      return _regeneratorRuntime.wrap(function _callee2$(_context2) {\n        while (1) {\n          switch (_context2.prev = _context2.next) {\n            case 0:\n              if (!(leaves.length === 1 && leaves[0].single && options.reduceSingleLeafToSelf)) {\n                _context2.next = 15;\n                break;\n              }\n\n              leaf = leaves[0];\n\n              if (!(leaf.cid.codec === 'raw' && (file.mtime !== undefined || file.mode !== undefined))) {\n                _context2.next = 14;\n                break;\n              }\n\n              _context2.next = 5;\n              return block.get(leaf.cid, options);\n\n            case 5:\n              _yield$block$get = _context2.sent;\n              _buffer = _yield$block$get.data;\n              leaf.unixfs = new UnixFS({\n                type: 'file',\n                mtime: file.mtime,\n                mode: file.mode,\n                data: _buffer\n              });\n              multihash = mh.decode(leaf.cid.multihash);\n              _buffer = new DAGNode(leaf.unixfs.marshal()).serialize();\n              _context2.next = 12;\n              return persist(_buffer, block, _objectSpread({}, options, {\n                codec: 'dag-pb',\n                hashAlg: multihash.name,\n                cidVersion: options.cidVersion\n              }));\n\n            case 12:\n              leaf.cid = _context2.sent;\n              leaf.size = _buffer.length;\n\n            case 14:\n              return _context2.abrupt(\"return\", {\n                cid: leaf.cid,\n                path: file.path,\n                unixfs: leaf.unixfs,\n                size: leaf.size\n              });\n\n            case 15:\n              // create a parent node and add all the leaves\n              f = new UnixFS({\n                type: 'file',\n                mtime: file.mtime,\n                mode: file.mode\n              });\n              links = leaves.filter(function (leaf) {\n                if (leaf.cid.codec === 'raw' && leaf.size) {\n                  return true;\n                }\n\n                if (leaf.unixfs && !leaf.unixfs.data && leaf.unixfs.fileSize()) {\n                  return true;\n                }\n\n                return Boolean(leaf.unixfs && leaf.unixfs.data && leaf.unixfs.data.length);\n              }).map(function (leaf) {\n                if (leaf.cid.codec === 'raw') {\n                  // node is a leaf buffer\n                  f.addBlockSize(leaf.size);\n                  return new DAGLink('', leaf.size, leaf.cid);\n                }\n\n                if (!leaf.unixfs || !leaf.unixfs.data) {\n                  // node is an intermediate node\n                  f.addBlockSize(leaf.unixfs && leaf.unixfs.fileSize() || 0);\n                } else {\n                  // node is a unixfs 'file' leaf node\n                  f.addBlockSize(leaf.unixfs.data.length);\n                }\n\n                return new DAGLink('', leaf.size, leaf.cid);\n              });\n              node = new DAGNode(f.marshal(), links);\n              buffer = node.serialize();\n              _context2.next = 21;\n              return persist(buffer, block, options);\n\n            case 21:\n              cid = _context2.sent;\n              return _context2.abrupt(\"return\", {\n                cid: cid,\n                path: file.path,\n                unixfs: f,\n                size: buffer.length + node.Links.reduce(function (acc, curr) {\n                  return acc + curr.Tsize;\n                }, 0)\n              });\n\n            case 23:\n            case \"end\":\n              return _context2.stop();\n          }\n        }\n      }, _callee2);\n    }));\n    return _reducer.apply(this, arguments);\n  }\n\n  return reducer;\n};\n/**\n * @type {import('../../types').UnixFSV1DagBuilder<File>}\n */\n\n\nfunction fileBuilder(file, block, options) {\n  var dagBuilder = dagBuilders[options.strategy];\n\n  if (!dagBuilder) {\n    throw errCode(new Error(\"Unknown importer build strategy name: \".concat(options.strategy)), 'ERR_BAD_STRATEGY');\n  }\n\n  return dagBuilder(buildFileBatch(file, block, options), reduce(file, block, options), options);\n}\n\nmodule.exports = fileBuilder;","map":null,"metadata":{},"sourceType":"script"}