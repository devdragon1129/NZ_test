{"ast":null,"code":"'use strict';\n\nvar defer = require('pull-defer');\n\nvar pull = require('pull-stream/pull');\n\nvar error = require('pull-stream/sources/error');\n\nvar values = require('pull-stream/sources/values');\n\nvar filter = require('pull-stream/throughs/filter');\n\nvar map = require('pull-stream/throughs/map');\n\nvar cat = require('pull-cat');\n\nvar Bucket = require('hamt-sharding/src/bucket');\n\nvar DirSharded = require('ipfs-unixfs-importer/src/importer/dir-sharded');\n\nvar waterfall = require('async/waterfall'); // Logic to export a unixfs directory.\n\n\nmodule.exports = shardedDirExporter;\n\nfunction shardedDirExporter(cid, node, name, path, pathRest, resolve, size, dag, parent, depth, options) {\n  var dir;\n\n  if (!parent || parent.path !== path) {\n    dir = {\n      name: name,\n      depth: depth,\n      path: path,\n      multihash: cid.buffer,\n      size: node.size,\n      type: 'dir'\n    };\n  } // we are at the max depth so no need to descend into children\n\n\n  if (options.maxDepth && options.maxDepth <= depth) {\n    return values([dir]);\n  }\n\n  if (!pathRest.length) {\n    // return all children\n    var streams = [pull(values(node.links), map(function (link) {\n      // remove the link prefix (2 chars for the bucket index)\n      var entryName = link.name.substring(2);\n      var entryPath = entryName ? path + '/' + entryName : path;\n      return {\n        depth: entryName ? depth + 1 : depth,\n        name: entryName,\n        path: entryPath,\n        multihash: link.cid.buffer,\n        pathRest: entryName ? pathRest.slice(1) : pathRest,\n        parent: dir || parent\n      };\n    }), resolve)]; // place dir before if not specifying subtree\n\n    streams.unshift(values([dir]));\n    return cat(streams);\n  }\n\n  var deferred = defer.source();\n  var targetFile = pathRest[0]; // recreate our level of the HAMT so we can load only the subshard in pathRest\n\n  waterfall([function (cb) {\n    if (!options.rootBucket) {\n      options.rootBucket = new Bucket({\n        hashFn: DirSharded.hashFn\n      });\n      options.hamtDepth = 1;\n      return addLinksToHamtBucket(node.links, options.rootBucket, options.rootBucket, cb);\n    }\n\n    return addLinksToHamtBucket(node.links, options.lastBucket, options.rootBucket, cb);\n  }, function (cb) {\n    return findPosition(targetFile, options.rootBucket, cb);\n  }, function (position, cb) {\n    var prefix = toPrefix(position.pos);\n    var bucketPath = toBucketPath(position);\n\n    if (bucketPath.length > options.hamtDepth) {\n      options.lastBucket = bucketPath[options.hamtDepth];\n      prefix = toPrefix(options.lastBucket._posAtParent);\n    }\n\n    var streams = [pull(values(node.links), map(function (link) {\n      var entryPrefix = link.name.substring(0, 2);\n      var entryName = link.name.substring(2);\n      var entryPath = entryName ? path + '/' + entryName : path;\n\n      if (entryPrefix !== prefix) {\n        // not the entry or subshard we're looking for\n        return false;\n      }\n\n      if (entryName && entryName !== targetFile) {\n        // not the entry we're looking for\n        return false;\n      }\n\n      if (!entryName) {\n        // we are doing to descend into a subshard\n        options.hamtDepth++;\n      } else {\n        // we've found the node we are looking for, remove the context\n        // so we don't affect further hamt traversals\n        delete options.rootBucket;\n        delete options.lastBucket;\n        delete options.hamtDepth;\n      }\n\n      return {\n        depth: entryName ? depth + 1 : depth,\n        name: entryName,\n        path: entryPath,\n        multihash: link.cid.buffer,\n        pathRest: entryName ? pathRest.slice(1) : pathRest,\n        parent: dir || parent\n      };\n    }), filter(Boolean), resolve)];\n\n    if (options.fullPath) {\n      streams.unshift(values([dir]));\n    }\n\n    cb(null, streams);\n  }], function (err, streams) {\n    if (err) {\n      return deferred.resolve(error(err));\n    }\n\n    deferred.resolve(cat(streams));\n  });\n  return deferred;\n}\n\nvar addLinksToHamtBucket = function addLinksToHamtBucket(links, bucket, rootBucket, callback) {\n  Promise.all(links.map(function (link) {\n    if (link.name.length === 2) {\n      var pos = parseInt(link.name, 16);\n      return bucket._putObjectAt(pos, new Bucket({\n        hashFn: DirSharded.hashFn\n      }, bucket, pos));\n    }\n\n    return rootBucket.put(link.name.substring(2), true);\n  })).catch(function (err) {\n    callback(err);\n    callback = null;\n  }).then(function () {\n    return callback && callback();\n  });\n};\n\nvar toPrefix = function toPrefix(position) {\n  return position.toString('16').toUpperCase().padStart(2, '0').substring(0, 2);\n};\n\nvar findPosition = function findPosition(file, bucket, cb) {\n  bucket._findNewBucketAndPos(file).catch(function (err) {\n    cb(err);\n    cb = null;\n  }).then(function (position) {\n    if (!cb) {\n      // would have errored in catch block above\n      return;\n    }\n\n    cb(null, position);\n  });\n};\n\nvar toBucketPath = function toBucketPath(position) {\n  var bucket = position.bucket;\n  var path = [];\n\n  while (bucket._parent) {\n    path.push(bucket);\n    bucket = bucket._parent;\n  }\n\n  path.push(bucket);\n  return path.reverse();\n};","map":null,"metadata":{},"sourceType":"script"}